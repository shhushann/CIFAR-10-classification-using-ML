{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ffd2490-810c-4005-a9af-d10a88b1bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac39bdfa-81f1-45c5-91ac-fb417a87fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load and preprocess\n",
    "def load_and_preprocess():\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset via Hugging Face Datasets, normalize and flatten it.\n",
    "\n",
    "    This function downloads the CIFAR-10 train and test splits, scales all image\n",
    "    pixel values from [0, 255] to [0.0, 1.0], and flattens each 32×32×3 image into\n",
    "    a 3072-dimensional vector.\n",
    "\n",
    "    Returns:\n",
    "        X_train (np.ndarray): Training features, shape (50000, 3072), dtype float32.\n",
    "        y_train (np.ndarray): Training labels, shape (50000,), dtype int64.\n",
    "        X_test  (np.ndarray): Test features, shape (10000, 3072), dtype float32.\n",
    "        y_test  (np.ndarray): Test labels, shape (10000,), dtype int64.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"cifar10\")\n",
    "    X_train = np.array(ds[\"train\"][\"img\"], dtype=np.float32) / 255.0\n",
    "    X_test  = np.array(ds[\"test\"][\"img\"],  dtype=np.float32) / 255.0\n",
    "    # X_train = np.array(ds[\"train\"][\"img\"]) / 255.0  # shape (50000,32,32,3)\n",
    "    y_train = np.array(ds[\"train\"][\"label\"])\n",
    "    # X_test  = np.array(ds[\"test\"][\"img\"])  / 255.0\n",
    "    y_test  = np.array(ds[\"test\"][\"label\"])\n",
    "    # flatten\n",
    "    X_train = X_train.reshape(len(X_train), -1)\n",
    "    X_test  = X_test.reshape(len(X_test),  -1)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c99ae0-1cad-4398-968a-dd81882e8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Simple MLP\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi-layer Perceptron classifier with ReLU hidden activations and softmax output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer_sizes : list of int\n",
    "        Sizes of each layer, including input and output layers.\n",
    "        E.g., [200, 256, 128, 10] means input dim 200 → hidden 256 → hidden 128 → output 10.\n",
    "    lr : float, optional (default=1e-2)\n",
    "        Learning rate for gradient descent updates.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : list of np.ndarray\n",
    "        Weight matrices for each layer, initialized with Xavier uniform.\n",
    "    biases : list of np.ndarray\n",
    "        Bias vectors for each layer, initialized to zeros.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(X)\n",
    "        Perform a forward pass through the network and return class probabilities.\n",
    "    backward(y_true)\n",
    "        Backpropagate the loss (softmax + cross-entropy) and update parameters.\n",
    "    train(X, y, epochs=10, batch_size=128)\n",
    "        Train the network on (X, y) using mini-batch gradient descent.\n",
    "    predict(X)\n",
    "        Compute class predictions for input data X.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes, lr=1e-2):\n",
    "        self.lr = lr\n",
    "        self.weights = []\n",
    "        self.biases  = []\n",
    "        # Xavier init\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            in_size, out_size = layer_sizes[i], layer_sizes[i+1]\n",
    "            bound = np.sqrt(6/(in_size+out_size))\n",
    "            W = np.random.uniform(-bound, bound, (in_size, out_size))\n",
    "            b = np.zeros(out_size)\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.activations = [X]\n",
    "        for i in range(len(self.weights)-1):\n",
    "            Z = self.activations[-1] @ self.weights[i] + self.biases[i]\n",
    "            A = np.maximum(0, Z)  # ReLU\n",
    "            self.activations.append(A)\n",
    "        # last layer\n",
    "        Z = self.activations[-1] @ self.weights[-1] + self.biases[-1]\n",
    "        # softmax\n",
    "        exps = np.exp(Z - Z.max(axis=1, keepdims=True))\n",
    "        A = exps / exps.sum(axis=1, keepdims=True)\n",
    "        self.activations.append(A)\n",
    "        return A\n",
    "\n",
    "    def backward(self, y_true):\n",
    "        grads_W = []\n",
    "        grads_b = []\n",
    "        m = y_true.shape[0]\n",
    "        # one-hot\n",
    "        Y = np.zeros_like(self.activations[-1])\n",
    "        Y[np.arange(m), y_true] = 1\n",
    "        # gradient on softmax-crossentropy\n",
    "        dA = (self.activations[-1] - Y) / m\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            A_prev = self.activations[i]\n",
    "            dW = A_prev.T @ dA\n",
    "            db = dA.sum(axis=0)\n",
    "            grads_W.insert(0, dW)\n",
    "            grads_b.insert(0, db)\n",
    "            if i>0:\n",
    "                dZ = dA @ self.weights[i].T\n",
    "                dA = dZ * (A_prev > 0)  # ReLU backward\n",
    "\n",
    "        # update\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.lr * grads_W[i]\n",
    "            self.biases[i]  -= self.lr * grads_b[i]\n",
    "\n",
    "    def train(self, X, y, epochs=10, batch_size=128):\n",
    "        n = X.shape[0]\n",
    "        for ep in range(epochs):\n",
    "            perm = np.random.permutation(n)\n",
    "            X, y = X[perm], y[perm]\n",
    "            t0 = time()\n",
    "            for i in range(0, n, batch_size):\n",
    "                xb = X[i:i+batch_size]\n",
    "                yb = y[i:i+batch_size]\n",
    "                self.forward(xb)\n",
    "                self.backward(yb)\n",
    "            # eval train loss/acc\n",
    "            probs = self.forward(X[:1000])\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            acc = (preds == y[:1000]).mean()\n",
    "            print(f\"Epoch {ep+1}/{epochs}  train‐acc@1k: {acc*100:.2f}%  time: {time()-t0:.1f}s\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f3b5f9c-b020-4a59-bab7-5762580e996a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a40a5738be4d789c0bf0eff67d4399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba2ed8bb547429882110004f813f60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/120M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcb541ca3a24ee4815416bc7c547763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/23.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd0e720125e4038a6607bb91e714862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcf7f836e3f4c6fa5039ac535487886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Running PCA...\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_and_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e4bfb8-87cd-44a9-bedf-06f3d44859b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "pca = PCA(n_components=200, svd_solver=\"auto\", whiten=False, random_state=42)\n",
    "pca.fit(X_train)\n",
    "X_train_p = pca.transform(X_train)\n",
    "X_test_p  = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b8bf52-264e-4488-8fce-62b3b18a2c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Training MLP on 200‐dim data...\n",
      "Epoch 1/20  train‐acc@1k: 28.10%  time: 0.4s\n",
      "Epoch 2/20  train‐acc@1k: 33.60%  time: 0.5s\n",
      "Epoch 3/20  train‐acc@1k: 36.00%  time: 0.5s\n",
      "Epoch 4/20  train‐acc@1k: 38.60%  time: 0.6s\n",
      "Epoch 5/20  train‐acc@1k: 39.20%  time: 0.5s\n",
      "Epoch 6/20  train‐acc@1k: 42.20%  time: 0.5s\n",
      "Epoch 7/20  train‐acc@1k: 42.90%  time: 0.5s\n",
      "Epoch 8/20  train‐acc@1k: 40.10%  time: 0.6s\n",
      "Epoch 9/20  train‐acc@1k: 45.50%  time: 0.6s\n",
      "Epoch 10/20  train‐acc@1k: 43.40%  time: 0.6s\n",
      "Epoch 11/20  train‐acc@1k: 43.10%  time: 0.6s\n",
      "Epoch 12/20  train‐acc@1k: 43.50%  time: 0.6s\n",
      "Epoch 13/20  train‐acc@1k: 43.00%  time: 0.6s\n",
      "Epoch 14/20  train‐acc@1k: 43.30%  time: 0.6s\n",
      "Epoch 15/20  train‐acc@1k: 44.50%  time: 0.6s\n",
      "Epoch 16/20  train‐acc@1k: 45.40%  time: 0.6s\n",
      "Epoch 17/20  train‐acc@1k: 47.40%  time: 0.5s\n",
      "Epoch 18/20  train‐acc@1k: 47.20%  time: 0.5s\n",
      "Epoch 19/20  train‐acc@1k: 47.70%  time: 0.5s\n",
      "Epoch 20/20  train‐acc@1k: 47.80%  time: 0.6s\n"
     ]
    }
   ],
   "source": [
    "print(\"▶ Training MLP on 200‐dim data...\")\n",
    "# architecture: 200 → 256 → 128 → 10\n",
    "mlp = MLP([200, 256, 128, 10], lr=1e-2)\n",
    "mlp.train(X_train_p, y_train, epochs=20, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5009cf99-60c7-472e-8ef6-7e6e52f14b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Evaluating on test set...\n",
      "Test accuracy: 46.98%\n"
     ]
    }
   ],
   "source": [
    "print(\"▶ Evaluating on test set...\")\n",
    "y_pred = mlp.predict(X_test_p)\n",
    "test_acc = (y_pred == y_test).mean()\n",
    "print(f\"Test accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c8f1fa-5ec7-404b-bee4-ebcfc640c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0  true: 3  pred: 3\n",
      "Sample 1  true: 8  pred: 8\n",
      "Sample 2  true: 8  pred: 8\n",
      "Sample 3  true: 0  pred: 8\n",
      "Sample 4  true: 6  pred: 4\n"
     ]
    }
   ],
   "source": [
    "# 5) sample predictions\n",
    "for i in range(5):\n",
    "    xi = X_test_p[i:i+1]\n",
    "    pred = mlp.predict(xi)[0]\n",
    "    print(f\"Sample {i}  true: {y_test[i]}  pred: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "033691d6-c340-47c9-9c83-2164112db1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "pca = PCA(n_components=300, svd_solver=\"auto\", whiten=False, random_state=42)\n",
    "pca.fit(X_train)\n",
    "X_train_p = pca.transform(X_train)\n",
    "X_test_p  = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8a1eefc-5a43-4a54-9b47-3b1b3a0376a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55db1f9e-012c-4dde-9652-4819a728fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# tell OpenBLAS / MKL to use up to 8 threads\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    Deep fully‐connected Perceptron network with:\n",
    "      - ReLU activations\n",
    "      - Softmax output & cross‐entropy loss with label smoothing\n",
    "      - Batch normalization on each hidden layer\n",
    "      - Dropout on hidden layers\n",
    "      - L2 weight decay\n",
    "      - Adam optimizer for all parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer_sizes : list of int\n",
    "        Sizes of each layer including input and output, e.g. [300, 512, 256, 128, 10].\n",
    "    lr : float, default=1e-3\n",
    "        Base learning rate for Adam.\n",
    "    dropout_rate : float in [0,1), default=0.5\n",
    "        Drop probability on hidden activations.\n",
    "    weight_decay : float, default=1e-4\n",
    "        L2 regularization coefficient.\n",
    "    label_smoothing : float in [0,1), default=0.1\n",
    "        ε for smoothing the one-hot targets: ŷ = (1–ε)·y + ε/K.\n",
    "    beta1, beta2 : floats, default=(0.9, 0.999)\n",
    "        Adam momentum hyperparameters.\n",
    "    eps : float, default=1e-8\n",
    "        Adam & batch‐norm stability constant.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes,\n",
    "                 lr=1e-3,\n",
    "                 dropout_rate=0.5,\n",
    "                 weight_decay=1e-4,\n",
    "                 label_smoothing=0.1,\n",
    "                 beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.lr = lr\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n",
    "        self.t = 0  # time step for Adam\n",
    "\n",
    "        L = len(layer_sizes) - 1  # number of layers\n",
    "        # Parameters\n",
    "        self.W = [None]*L\n",
    "        self.b = [None]*L\n",
    "   \n",
    "        self.gamma = [None]*(L-1)\n",
    "        self.beta = [None]*(L-1)\n",
    "        # Adam moment estimates\n",
    "        self.mW = []; self.vW = []\n",
    "        self.mb = []; self.vb = []\n",
    "        self.mg = []; self.vg = []\n",
    "        self.mb_bn = []; self.vb_bn = []\n",
    "\n",
    "        for i in range(L):\n",
    "            in_dim, out_dim = layer_sizes[i], layer_sizes[i+1]\n",
    "            bound = np.sqrt(6/(in_dim + out_dim))\n",
    "             # create and cast here\n",
    "            W_i = np.random.uniform(-bound, bound, (in_dim, out_dim)).astype(np.float32)\n",
    "            b_i = np.zeros(out_dim, dtype=np.float32)\n",
    "\n",
    "            self.W[i] = W_i\n",
    "            self.b[i] = b_i\n",
    "            \n",
    "            self.mW.append(np.zeros_like(self.W[i]))\n",
    "            self.vW.append(np.zeros_like(self.W[i]))\n",
    "            self.mb.append(np.zeros_like(self.b[i]))\n",
    "            self.vb.append(np.zeros_like(self.b[i]))\n",
    "            if i < L-1:\n",
    "                # batch-norm params for hidden layers\n",
    "                self.gamma[i] = np.ones(out_dim)\n",
    "                self.beta[i]  = np.zeros(out_dim)\n",
    "                self.mg.append(np.zeros_like(self.gamma[i]))\n",
    "                self.vg.append(np.zeros_like(self.gamma[i]))\n",
    "                self.mb_bn.append(np.zeros_like(self.beta[i]))\n",
    "                self.vb_bn.append(np.zeros_like(self.beta[i]))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass with batch‐norm + ReLU + dropout.\n",
    "        Returns softmax probabilities.\n",
    "        \"\"\"\n",
    "        self.cache = {'A':[X], 'bn':[]}\n",
    "        A = X\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(self.W)-1):\n",
    "            Z = A @ self.W[i] + self.b[i]                        # linear\n",
    "            mu = Z.mean(axis=0, keepdims=True)\n",
    "            var = Z.var(axis=0, keepdims=True)\n",
    "            inv_std = 1/np.sqrt(var + self.eps)\n",
    "            Z_norm = (Z - mu)*inv_std\n",
    "            # affine transform\n",
    "            Z_tilde = self.gamma[i]*Z_norm + self.beta[i]\n",
    "            A = np.maximum(0, Z_tilde)                           # ReLU\n",
    "            # dropout\n",
    "            if training and self.dropout_rate>0:\n",
    "                mask = (np.random.rand(*A.shape) > self.dropout_rate)/(1-self.dropout_rate)\n",
    "                A *= mask\n",
    "            else:\n",
    "                mask = None\n",
    "            # store caches\n",
    "            self.cache['A'].append(A)\n",
    "            self.cache['bn'].append((Z, Z_norm, mu, inv_std, self.gamma[i], mask))\n",
    "\n",
    "        # Output layer\n",
    "        Z_out = A @ self.W[-1] + self.b[-1]\n",
    "        exps = np.exp(Z_out - Z_out.max(axis=1, keepdims=True))\n",
    "        probs = exps / exps.sum(axis=1, keepdims=True)\n",
    "        self.cache['A'].append(probs)\n",
    "        return probs\n",
    "\n",
    "    def backward(self, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass through softmax‐CE (with label smoothing), then\n",
    "        output layer, then hidden layers (dropout → ReLU → BN → linear).\n",
    "        Updates all params via Adam.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        K = self.layer_sizes[-1]\n",
    "        # make smoothed targets\n",
    "        Y = np.zeros((m, K))\n",
    "        Y[np.arange(m), y_true] = 1\n",
    "        Y = (1 - self.label_smoothing)*Y + self.label_smoothing/K\n",
    "\n",
    "        # dL/dZ_out\n",
    "        probs = self.cache['A'][-1]\n",
    "        dZ = (probs - Y) / m\n",
    "\n",
    "        # gradients for output layer\n",
    "        A_prev = self.cache['A'][-2]\n",
    "        dW = A_prev.T @ dZ + self.weight_decay * self.W[-1]\n",
    "        db = dZ.sum(axis=0)\n",
    "        self._adam_update('W', -1, dW, db)\n",
    "\n",
    "        # backprop into hidden layers\n",
    "        dA = dZ @ self.W[-1].T\n",
    "        for i in reversed(range(len(self.W)-1)):\n",
    "            Z, Z_norm, mu, inv_std, gamma_i, mask = self.cache['bn'][i]\n",
    "            A_prev = self.cache['A'][i]\n",
    "\n",
    "            # dropout backward\n",
    "            if mask is not None:\n",
    "                dA *= mask\n",
    "            # ReLU backward\n",
    "            dZ_tilde = dA * (Z_norm * inv_std * 0 + 1)  # dummy to hint shape\n",
    "            dZ_tilde = dA * (self.cache['A'][i+1] > 0)\n",
    "\n",
    "            # BN backward\n",
    "            N = Z.shape[0]\n",
    "            dgamma = np.sum(dZ_tilde * Z_norm, axis=0)\n",
    "            dbeta  = np.sum(dZ_tilde, axis=0)\n",
    "            dZ_norm = dZ_tilde * gamma_i\n",
    "            dvar = np.sum(dZ_norm * (Z - mu) * -0.5 * inv_std**3, axis=0, keepdims=True)\n",
    "            dmu  = np.sum(dZ_norm * -inv_std, axis=0, keepdims=True) + \\\n",
    "                   dvar * np.mean(-2*(Z-mu), axis=0, keepdims=True)\n",
    "            dZ_bn = dZ_norm * inv_std + dvar*2*(Z-mu)/N + dmu/N\n",
    "\n",
    "            # linear layer grads\n",
    "            dW = A_prev.T @ dZ_bn + self.weight_decay * self.W[i]\n",
    "            db = dZ_bn.sum(axis=0)\n",
    "\n",
    "            # Adam update for W,b,gamma,beta\n",
    "            self._adam_update('W', i, dW, db)\n",
    "            self._adam_update('BN', i, dgamma, dbeta)\n",
    "\n",
    "            # propagate to next\n",
    "            dA = dZ_bn @ self.W[i].T\n",
    "\n",
    "    def _adam_update(self, kind, idx, grad_W, grad_b):\n",
    "        \"\"\"Helper to update either ('W',b) or ('BN',beta).\"\"\"\n",
    "        self.t += 1\n",
    "        if kind == 'W':\n",
    "            # weights\n",
    "            self.mW[idx] = self.beta1*self.mW[idx] + (1-self.beta1)*grad_W\n",
    "            self.vW[idx] = self.beta2*self.vW[idx] + (1-self.beta2)*(grad_W**2)\n",
    "            m_hat = self.mW[idx]/(1-self.beta1**self.t)\n",
    "            v_hat = self.vW[idx]/(1-self.beta2**self.t)\n",
    "            self.W[idx] -= self.lr * m_hat/(np.sqrt(v_hat)+self.eps)\n",
    "            # biases\n",
    "            self.mb[idx] = self.beta1*self.mb[idx] + (1-self.beta1)*grad_b\n",
    "            self.vb[idx] = self.beta2*self.vb[idx] + (1-self.beta2)*(grad_b**2)\n",
    "            m_hat_b = self.mb[idx]/(1-self.beta1**self.t)\n",
    "            v_hat_b = self.vb[idx]/(1-self.beta2**self.t)\n",
    "            self.b[idx]  -= self.lr * m_hat_b/(np.sqrt(v_hat_b)+self.eps)\n",
    "        else:\n",
    "            # batch-norm gamma & beta\n",
    "            self.mg[idx] = self.beta1*self.mg[idx] + (1-self.beta1)*grad_W\n",
    "            self.vg[idx] = self.beta2*self.vg[idx] + (1-self.beta2)*(grad_W**2)\n",
    "            mg_hat = self.mg[idx]/(1-self.beta1**self.t)\n",
    "            vg_hat = self.vg[idx]/(1-self.beta2**self.t)\n",
    "            self.gamma[idx] -= self.lr * mg_hat/(np.sqrt(vg_hat)+self.eps)\n",
    "            self.mb_bn[idx] = self.beta1*self.mb_bn[idx] + (1-self.beta1)*grad_b\n",
    "            self.vb_bn[idx] = self.beta2*self.vb_bn[idx] + (1-self.beta2)*(grad_b**2)\n",
    "            mbn_hat = self.mb_bn[idx]/(1-self.beta1**self.t)\n",
    "            vbn_hat = self.vb_bn[idx]/(1-self.beta2**self.t)\n",
    "            self.beta[idx]  -= self.lr * mbn_hat/(np.sqrt(vbn_hat)+self.eps)\n",
    "\n",
    "    def train(self, X, y, epochs=100, batch_size=128):\n",
    "        \"\"\"Mini-batch training loop with periodic train-subset accuracy print.\"\"\"\n",
    "        n = X.shape[0]\n",
    "        for ep in range(1, epochs+1):\n",
    "            perm = np.random.permutation(n)\n",
    "            Xs, ys = X[perm], y[perm]\n",
    "            for i in range(0, n, batch_size):\n",
    "                xb, yb = Xs[i:i+batch_size], ys[i:i+batch_size]\n",
    "                self.forward(xb, training=True)\n",
    "                self.backward(yb)\n",
    "            # monitor\n",
    "            probs = self.forward(X[:1000], training=False)\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            acc = (preds == y[:1000]).mean()*100\n",
    "            print(f\"Epoch {ep}/{epochs} — acc@1k: {acc:.2f}%\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return predicted class indices for X.\"\"\"\n",
    "        probs = self.forward(X, training=False)\n",
    "        return np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13f9938-a87c-4ab8-8941-5553cc5597a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_and_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0823e187-66e6-4ae2-86a8-92110cc604f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "pca = PCA(n_components=300, svd_solver=\"auto\", whiten=False, random_state=42)\n",
    "pca.fit(X_train)\n",
    "X_train_p = pca.transform(X_train)\n",
    "X_test_p  = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eda9a31-9627-4909-a817-4e19bdfdd787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Training MLP on 500‐dim data...\n",
      "Epoch 1/80 — acc@1k: 52.30%\n",
      "Epoch 2/80 — acc@1k: 59.10%\n",
      "Epoch 3/80 — acc@1k: 61.30%\n",
      "Epoch 4/80 — acc@1k: 67.20%\n",
      "Epoch 5/80 — acc@1k: 67.50%\n",
      "Epoch 6/80 — acc@1k: 72.60%\n",
      "Epoch 7/80 — acc@1k: 74.10%\n",
      "Epoch 8/80 — acc@1k: 73.70%\n",
      "Epoch 9/80 — acc@1k: 76.30%\n",
      "Epoch 10/80 — acc@1k: 77.40%\n",
      "Epoch 11/80 — acc@1k: 80.90%\n",
      "Epoch 12/80 — acc@1k: 79.70%\n",
      "Epoch 13/80 — acc@1k: 83.90%\n",
      "Epoch 14/80 — acc@1k: 83.40%\n",
      "Epoch 15/80 — acc@1k: 84.60%\n",
      "Epoch 16/80 — acc@1k: 83.60%\n",
      "Epoch 17/80 — acc@1k: 85.80%\n",
      "Epoch 18/80 — acc@1k: 87.20%\n",
      "Epoch 19/80 — acc@1k: 87.20%\n",
      "Epoch 20/80 — acc@1k: 87.10%\n",
      "Epoch 21/80 — acc@1k: 88.90%\n",
      "Epoch 22/80 — acc@1k: 88.60%\n",
      "Epoch 23/80 — acc@1k: 90.60%\n",
      "Epoch 24/80 — acc@1k: 91.00%\n",
      "Epoch 25/80 — acc@1k: 89.80%\n",
      "Epoch 26/80 — acc@1k: 90.40%\n",
      "Epoch 27/80 — acc@1k: 92.30%\n",
      "Epoch 28/80 — acc@1k: 91.40%\n",
      "Epoch 29/80 — acc@1k: 90.90%\n",
      "Epoch 30/80 — acc@1k: 91.50%\n",
      "Epoch 31/80 — acc@1k: 91.70%\n",
      "Epoch 32/80 — acc@1k: 90.80%\n",
      "Epoch 33/80 — acc@1k: 92.70%\n",
      "Epoch 34/80 — acc@1k: 93.40%\n",
      "Epoch 35/80 — acc@1k: 92.00%\n",
      "Epoch 36/80 — acc@1k: 93.10%\n",
      "Epoch 37/80 — acc@1k: 93.80%\n",
      "Epoch 38/80 — acc@1k: 93.70%\n",
      "Epoch 39/80 — acc@1k: 93.20%\n",
      "Epoch 40/80 — acc@1k: 94.00%\n",
      "Epoch 41/80 — acc@1k: 94.10%\n",
      "Epoch 42/80 — acc@1k: 93.80%\n",
      "Epoch 43/80 — acc@1k: 92.70%\n",
      "Epoch 44/80 — acc@1k: 94.10%\n",
      "Epoch 45/80 — acc@1k: 94.10%\n",
      "Epoch 46/80 — acc@1k: 94.20%\n",
      "Epoch 47/80 — acc@1k: 94.40%\n",
      "Epoch 48/80 — acc@1k: 93.40%\n",
      "Epoch 49/80 — acc@1k: 94.80%\n",
      "Epoch 50/80 — acc@1k: 95.10%\n",
      "Epoch 51/80 — acc@1k: 94.20%\n",
      "Epoch 52/80 — acc@1k: 93.20%\n",
      "Epoch 53/80 — acc@1k: 94.10%\n",
      "Epoch 54/80 — acc@1k: 95.10%\n",
      "Epoch 55/80 — acc@1k: 93.90%\n",
      "Epoch 56/80 — acc@1k: 94.80%\n",
      "Epoch 57/80 — acc@1k: 93.90%\n",
      "Epoch 58/80 — acc@1k: 94.80%\n",
      "Epoch 59/80 — acc@1k: 94.80%\n",
      "Epoch 60/80 — acc@1k: 94.40%\n",
      "Epoch 61/80 — acc@1k: 95.70%\n",
      "Epoch 62/80 — acc@1k: 94.70%\n",
      "Epoch 63/80 — acc@1k: 95.30%\n",
      "Epoch 64/80 — acc@1k: 94.30%\n",
      "Epoch 65/80 — acc@1k: 96.00%\n",
      "Epoch 66/80 — acc@1k: 95.00%\n",
      "Epoch 67/80 — acc@1k: 94.90%\n",
      "Epoch 68/80 — acc@1k: 94.60%\n",
      "Epoch 69/80 — acc@1k: 95.80%\n",
      "Epoch 70/80 — acc@1k: 95.20%\n",
      "Epoch 71/80 — acc@1k: 95.00%\n",
      "Epoch 72/80 — acc@1k: 96.20%\n",
      "Epoch 73/80 — acc@1k: 94.20%\n",
      "Epoch 74/80 — acc@1k: 94.90%\n",
      "Epoch 75/80 — acc@1k: 95.20%\n",
      "Epoch 76/80 — acc@1k: 96.30%\n",
      "Epoch 77/80 — acc@1k: 95.20%\n",
      "Epoch 78/80 — acc@1k: 96.10%\n",
      "Epoch 79/80 — acc@1k: 95.50%\n",
      "Epoch 80/80 — acc@1k: 94.90%\n"
     ]
    }
   ],
   "source": [
    "print(\"▶ Training MLP on 300‐dim data...\")\n",
    "# architecture: 200 → 256 → 128 → 10\n",
    "mlp = MLP(\n",
    "  layer_sizes=[300, 1024, 512, 256, 10],  # one extra wide hidden layer\n",
    "  lr=3e-3,                                      # bump initial LR\n",
    "  dropout_rate=0.3,                             # less aggressive dropout\n",
    "  weight_decay=5e-5,                            # lighter L2\n",
    "  label_smoothing=0.1\n",
    ")\n",
    "\n",
    "\n",
    "mlp.train(X_train_p, y_train, epochs=80, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9a2bc5-2b9c-4083-9791-7a4c03418839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Evaluating on test set...\n",
      "Test accuracy: 58.56%\n"
     ]
    }
   ],
   "source": [
    "print(\"▶ Evaluating on test set...\")\n",
    "y_pred = mlp.predict(X_test_p)\n",
    "test_acc = (y_pred == y_test).mean()\n",
    "print(f\"Test accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da1912-da58-47fa-9560-f07f7b8186f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
